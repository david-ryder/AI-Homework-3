{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzo37yE6ejuA"
      },
      "source": [
        "# Convolution Neural Network Homework\n",
        "\n",
        "This is the 3rd homework assignment for CAP 4630 and we will go through some primary operations for image processsing and implement one of the earilest representative convolution neural network - LeNet-5 . \\\n",
        "You will use **\"Tasks\"** and **\"Hints\"** to finish the work. **(Total 100 Points)** \\\n",
        "For section 1, when you implement covolution and maxpooling, you are **not** allowed to use built-in functions in Machine Learning libaries such as Scikit-learn Keras, Tensorflow, Pytorch; but you are encouraged to employ Keras for second section.\n",
        "\n",
        "**Task Overview:**\n",
        "- Basic operations for Digital Image Processing (DIP)\n",
        "- LeNet-5 (Google Colab is recommended for implementation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLKjpQvaejuF"
      },
      "source": [
        "## 1 - Basic Image Processing ##\n",
        "### 1.1 Data Preparation \n",
        "\n",
        "Import packages and prepare image data as an array for image processing. **(5 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Import numpy and rename it to np.\n",
        "2. Import imageio and call imread to convert image to an array.\n",
        "3. **DISPLAY** the image in the output box before image-array conversion.\n",
        "4. **PRINT OUT** the size of the array\n",
        "5. **PRINT OUT** the numeric matrix form of image, i.e. the obtained array after image-array conversion.\n",
        "\n",
        "References:\n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [imageio](https://imageio.github.io/) is a python library for basic image reading and writing.\n",
        "\n",
        "**Hints:**\n",
        "1. Image data is under current directory, i.e., \"./image.jpg\".\n",
        "2. You may consider importing \"display\" and \"Image\" from IPython.display for image display."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "qvITLRocejuG"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAeAB4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AGfD7w+l5qkV5qJkjt0MkYRpyfOYrjAVuvU8/wCz2r0m+sPC+mSwQXdlpqSybvIW4UHcR7t1PSuW1C8srPUrSG2s7W3s7h1jP2dRxIwc5wD0wnp3ry/xPpk9trEvnFWSb95G6nhl6fn7V32j6v4a0TVLGxaZJJ7eSRLWSU7/ACnJbJLd2O5h09O9ReLZdP17UJW1G3t5ry3BjiEhYqBk9SMY/I1zVzbhrGyhEvlxI7vhOhbAGM9sA4H4+tRajqtqLSNLyza7dHKiQNsVR6A/j09qs+KPBN/J4mfUtMtFkjlIkIjYfI/8R7dxu71KY7rTJJluiVuZFAkRlyxzz1xx65rJn1BLSB/tYzGASsbOV+bGMnBrkbjUlN60lrM6llAYglc+/Wvo9VX7sih0Y4ZT3Fc78QtDtoNEkvLdSkkEzQNlidygDB9u3HT8ufD5IZb+6ZXlPl5yQf8APNJZabDdyyCL5AvQsM1//9k=",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(30, 30)\n",
            "[[ 98  89  78 112  90  93 158 112 122 117 107 121  90  75  67  93  73  67\n",
            "   96 152 167 151 166 154 134 105  53  51  55  44]\n",
            " [ 34  31  51  38  16  50 195 161 138 173 200 209 194 206 222 237 223 195\n",
            "  177 185 167 160 166 169 136  98  49  54  55  35]\n",
            " [ 78  84  94  71  81  69 190 204 233 240 249 234 232 241 250 235 235 236\n",
            "  224 194 169 181 170 162 121  95  55  49  52  47]\n",
            " [ 79  58  90 127 103 137 253 244 254 222 223 219 235 228 232 206 201 215\n",
            "  201 106 105 137 169 173 124  81  45  49  59  53]\n",
            " [ 31  75  44  81  89 191 242 255 228 208 231 233 236 222 227 199 177 192\n",
            "  208 104 137 109 170 194 159  88  44  54  63  46]\n",
            " [ 85  66  51 145 129 225 239 206 242 233 247 231 210 218 232 202 190 173\n",
            "  181  95 146  39 132 199 178 145 103  53  44  65]\n",
            " [ 81  60  76 178 216 214 209 230 240 229 230 211 157 176 214 218 185 186\n",
            "  198 154 203  33 100 157 157 179 162  72  39  78]\n",
            " [104  89 152 176 168 117 182 226 242 235 238 207  82  50  94 145 136 157\n",
            "  148 119 175  30 111 165 136 155 168 110  60  56]\n",
            " [ 95  39 181 160  91  13 180 237 240 242 221 102  24   7  19  12  38  95\n",
            "  101 142 155  30  89  91 107 145 161 113  60  53]\n",
            " [ 18  54 186 144 147  99 248 238 235 227 210 115  62  57  74  72  85  82\n",
            "   91 182  83  56  68  59 129 113 133 129 116  67]\n",
            " [ 84  79 201 132 185 229 217 206 210 188 188 155 160 169 164 141 111  70\n",
            "  126 154  49  40  69  73 104  85 120 132 158 125]\n",
            " [201 161 135 153 174 219 204 197 209 178 177 146 138 124 114 101  90  85\n",
            "  144 105  81  53  83 104  74  80 122 141 152 133]\n",
            " [162 150 147 212 176 180 190 187 161 107 114 140 191 178 114  49  74 117\n",
            "  112 118  99 114  93  98  96  90 104 139 102  67]\n",
            " [184 138 167 132  90 118 183 214 193 150 148 132 137 130 135 141  96 127\n",
            "  108 140  75 111 113  96 115 101  75  89  42  43]\n",
            " [172 127 105  83  61  34 134 149 188 134 119  97 106 106 115 120 109 107\n",
            "  123 139  70  77 110 103 102  91  67  68  42  61]\n",
            " [139  39  59  55  36  15  27 128 189 123  98  79 100 105 105  97  95  86\n",
            "  105 148  94  95  78  92  93  67  78 110  94  51]\n",
            " [162  27  19  24   2  34  19 104 153  91  88  71 100 151 122 118  98  96\n",
            "   92 143 136  86  92  79  83  64  35  31  61  87]\n",
            " [186 114  41  35  15  20  37  75 101  92  91  86 122 172 147  97  85  96\n",
            "   86 104  88  61  80  64  68  65  37  12  21  38]\n",
            " [183 164  24  17  41  42  75  49  67  80  74  86  98 104 116  99  81 101\n",
            "   90  94  97 116 149 125  69  83  64  30  28  42]\n",
            " [172 207  94  39  34  18  50  52  82  95  92  99  97  86 104 120  82  88\n",
            "   69  66  82 112 126  84  52  66  53  25  20  26]\n",
            " [159 164 106  37  51  56  38  56  62  87 101  90 108 125  99  84  94  90\n",
            "   78  79  86  97  89  48  79  77  59  41  34  27]\n",
            " [179 181 191 103  67  52  28  77  92 109 121 113 129 129  93  93  65  66\n",
            "   77  86  83  79  71  52  55  46  30  24  26  19]\n",
            " [198 178 191 158 154 149 142 139 144 144 109 102 103  73  72  98  89  84\n",
            "   94  92  76  65  51  40  19  21  12   9  19  25]\n",
            " [173 168 176 182 184 172 208 171 198 207 118  91 101  88 107  93 109  92\n",
            "   87  72  55  47  26   6   9  21  14   1   9  19]\n",
            " [180 180 176 177 187 189 165 134 130 188 137 113  70 107  79  61  81  61\n",
            "   47  49  48  30  10   0   4   5   5   5   4   7]\n",
            " [176 177 176 178 186 190 183 172 160 151 124 134 109  74  58  64  56  48\n",
            "   42  37  23   8   8  17   9   9  11  14  18  23]\n",
            " [175 174 176 180 181 185 195 207 179 208 217 168 122  92  94  48  52  34\n",
            "   18  14  12  10  14  21  33  28  23  23  26  28]\n",
            " [179 174 177 182 178 175 189 210 216 220 204 246 134 106  65  57  55  34\n",
            "   14   9  15  21  27  32  54  43  31  29  34  36]\n",
            " [183 176 177 185 182 172 179 196 255 153 188 221 225  84  82  51  42  39\n",
            "   33  25  19  23  41  59  49  37  27  31  47  59]\n",
            " [184 177 179 188 186 177 177 186 242 187 172 247 199 148  91  54  40  35\n",
            "   30  29  34  44  55  63  34  24  18  28  51  71]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\drw12\\AppData\\Local\\Temp\\ipykernel_23960\\3668959875.py:13: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  img_matrix = imageio.imread('./image.jpg')\n"
          ]
        }
      ],
      "source": [
        "# Import useful libraries\n",
        "import numpy as np\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Image\n",
        "import math\n",
        "\n",
        "# Display original image\n",
        "image = Image(filename='./image.jpg')\n",
        "display(image)\n",
        "\n",
        "# # Convert image to array, print out the shape of array, and print out the entire array\n",
        "img_matrix = imageio.imread('./image.jpg')\n",
        "print(img_matrix.shape)\n",
        "print(img_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY5djq-tejuI"
      },
      "source": [
        "### 1.2 Implementation of Convolution Filter\n",
        "\n",
        "Process the obtained array from the image with convolution operation. **(20 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Prepare a 3X3 Laplacian kernel (aka Laplacial filter) with array as convolution filter.\n",
        "2. Conduct convolution on image with prepared kernel.\n",
        "3. **PRINT OUT** convolution result for first ten rows.\n",
        "4. **PRINT OUT** the shape of the convolution result.\n",
        "5. **DISPLAY** convolution result as image with matplotlib. (Don't worry about the value <0 or >255. Scaling process will be conducted in imshow function to make sure valid display.)\n",
        "\n",
        "\n",
        "**Hints:**\n",
        "1. Laplacian kernel is widely used for edge detection. Its form is shown below:\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=15bP8KCwHLtglJ-WXV4wolm4m46mCp3HL)\n",
        "\n",
        "2. You may consider the following steps for this implementation.\\\n",
        "    2.1 Extract all centriods of processing region for each convolution operation.\\\n",
        "    2.2 According to each centroid, locate all indices of the elements within the local region for each convolution operation.\\\n",
        "    2.3 Given obtained indices, locate pixel values (i.e. our obtained array elements) and conduct element-wise product between pixel and kernel values.\\\n",
        "    2.4 Sum element-wise product results and assign the value to convolution result at corresponding location.\\\n",
        "    **Note: we did not conduct padding for processed array, and thus, convolution result will become smaller than original array. You may think about the reason.**\n",
        "3. Validation for first 5X5 array (from upper-left corner), i.e., filtered_results[0:5,0:5]. The example figure is below.\n",
        "\n",
        "[[ 134.   37.   98.  195.  173.]\\\n",
        " [ -75.  -80.   56.  -65.  182.]\\\n",
        " [  96.  -37. -163.   22.   68.]\\\n",
        " [-101.  121.   81.  148.  -71.]\\\n",
        " [   7.  127. -141.  159. -127.]]\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=18Iis1mJsvEaojZ7O3f3soE152Szwy8_Z)\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "cnc8BNW9ejuJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 134.   37.   98.  195.  173. -221.    5.  137.    3.  -62.  -87.  -39.\n",
            "   -92. -128. -175. -152.  -77.   -8.  -50.   13.   25.    1.  -58.  -22.\n",
            "    -7.   64.  -12.  -24.]\n",
            " [ -75.  -80.   56.  -65.  182.  -39.   12.  -96.  -83.  -99.  -27.  -24.\n",
            "   -48.  -70.  -12.  -45.  -75.  -88.  -92.  -29.  -88.   -2.  -15.   33.\n",
            "   -25.   18.   14.    2.]\n",
            " [  96.  -37. -163.   22.   68. -199.  -10.  -89.   37.   29.   49.  -25.\n",
            "    18.  -17.   43.   29.  -30.  -51.  180.  129.   16.  -26.  -43.   38.\n",
            "    28.   49.   11.  -19.]\n",
            " [-101.  121.   81.  148.  -71.  -30. -100.   47.   82.  -13.  -15.  -44.\n",
            "    21.  -23.   16.   74.    5. -154.  130.  -84.   47.  -76.  -75.  -52.\n",
            "    77.  114.   -7.  -49.]\n",
            " [   7.  127. -141.  159. -127.  -74.  142.  -61.   -6.  -63.  -23.    2.\n",
            "   -32.  -67.   31.  -23.   57.  -50.  205. -110.  264.  -20. -135.  -52.\n",
            "   -32.   -8.   61.   44.]\n",
            " [  72.  137.  -99. -175.  -89.   29.  -39.  -17.   22.    5.  -19.   51.\n",
            "   -65. -136. -126.  -10.  -31. -123.   -1. -304.  240.   33.   -7.   22.\n",
            "   -97. -126.   76.   98.]\n",
            " [  -1.  -86.  -46.  -72.  109.    4.  -13.  -27.   11.  -59. -195.  110.\n",
            "   159.   52. -120.  -19.  -63.  -17.  143. -193.  229.  -60. -165.   40.\n",
            "     8.  -84.  -27.   25.]\n",
            " [ 263. -187.  -48.  124.  435.  -40.  -64.   -4.  -45.  -92.  159.  157.\n",
            "   122.  111.  226.  176.   -2.   72.  -11. -190.  210.  -56.   56.   73.\n",
            "   -44.  -85.    8.  102.]\n",
            " [ 106. -164.   49.  -69.  241. -258.  -26.  -25.  -33.  -89.   69.  108.\n",
            "    84.   16.   24.  -37.   13.  127. -258.  110.   -3.    1.  125. -133.\n",
            "    40.   -9.  -22.  -50.]\n",
            " [ 184. -272.  155.  -58. -196.   19.   38.   -2.   51.  -22.  -11. -116.\n",
            "  -171. -158. -116.  -58.  124.  -45. -154.  162.   67.  -12.   44.  -55.\n",
            "    77.   -8.   20. -107.]]\n",
            "(28, 28)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x20feedc9de0>"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "######## Convolution with Laplacian Filter ##################\n",
        "l_filter = np.array([\n",
        "    [0, 1, 0], \n",
        "    [1, -4, 1], \n",
        "    [0, 1, 0]\n",
        "])\n",
        "\n",
        "def calculate_target_size(img_size: int, kernel_size: int) -> int:\n",
        "    num_pixels = 0\n",
        "    \n",
        "    # From 0 up to img size (if img size = 224, then up to 223)\n",
        "    for i in range(img_size):\n",
        "        # Add the kernel size (let's say 3) to the current i\n",
        "        added = i + kernel_size\n",
        "        # It must be lower than the image size\n",
        "        if added <= img_size:\n",
        "            # Increment if so\n",
        "            num_pixels += 1\n",
        "            \n",
        "    return num_pixels\n",
        "\n",
        "def convolve(img: np.array, kernel: np.array) -> np.array:\n",
        "    # Assuming a rectangular image\n",
        "    tgt_size = calculate_target_size(\n",
        "        img_size=img.shape[0],\n",
        "        kernel_size=kernel.shape[0]\n",
        "    )\n",
        "    # To simplify things\n",
        "    k = kernel.shape[0]\n",
        "    \n",
        "    # 2D array of zeros\n",
        "    convolved_img = np.zeros(shape=(tgt_size, tgt_size))\n",
        "    \n",
        "    # Iterate over the rows\n",
        "    for i in range(tgt_size):\n",
        "        # Iterate over the columns\n",
        "        for j in range(tgt_size):\n",
        "            # img[i, j] = individual pixel value\n",
        "            # Get the current matrix\n",
        "            mat = img[i:i+k, j:j+k]\n",
        "            \n",
        "            # Apply the convolution - element-wise multiplication and summation of the result\n",
        "            # Store the result to i-th row and j-th column of our convolved_img array\n",
        "            convolved_img[i, j] = np.sum(np.multiply(mat, kernel))\n",
        "            \n",
        "    return convolved_img\n",
        "\n",
        "img_convolved = convolve(img=np.array(img_matrix), kernel=l_filter)\n",
        "\n",
        "print(img_convolved[:10])\n",
        "\n",
        "print(img_convolved.shape)\n",
        "\n",
        "plt.imshow(img_convolved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMN_XE1xejuJ"
      },
      "source": [
        "### 1.3 Modification on Convolution Scheme\n",
        "\n",
        "Conduct the convolution with the same filter as above, but change the stride to 2. **(5 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Modify the convolution process with stride=2\n",
        "2. **PRINT OUT** convolution result for first ten rows.\n",
        "3. **PRINT OUT** the shape of the convolution result.\n",
        "4. **DISPLAY** convolution result as image with matplotlib.(Don't worry about the value <0 or >255. Scaling process will be conducted in imshow function to make sure valid display.)\n",
        "\n",
        "**Hints:**\n",
        "1. You may just reduce the centroid pool according to stride=2, and then, follow the same convolution process above.\n",
        "    **Note: After increase of stride, the size of convolution result is further shrinked. You may think about the reason.**\n",
        "2. Validation for first 5X5 array (from upper-left corner), i.e., filtered_results[0:5,0:5]. The example figure is below.\n",
        "\n",
        "[[ 134.   98.  173.    5.    3.]\\\n",
        " [  96. -163.   68.  -10.   37.]\\\n",
        " [   7. -141. -127.  142.   -6.]\\\n",
        " [  -1.  -46.  109.  -13.   11.]\\\n",
        " [ 106.   49.  241.  -26.  -33.]]\n",
        " \n",
        " \n",
        "![](https://drive.google.com/uc?export=view&id=1UPdXt5cY1umImu2chaQLfWAnqDEpFOGV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "pyFLwZxYejuJ"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'float' object cannot be interpreted as an integer",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [69], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m             convolved_img[i, j] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mmultiply(mat, kernel))\n\u001b[0;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m convolved_img\n\u001b[1;32m---> 29\u001b[0m img_convolved2 \u001b[39m=\u001b[39m convolve_n(img\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marray(img_matrix), kernel\u001b[39m=\u001b[39ml_filter, n\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(img_convolved2[:\u001b[39m10\u001b[39m])\n\u001b[0;32m     33\u001b[0m \u001b[39mprint\u001b[39m(img_convolved2\u001b[39m.\u001b[39mshape)\n",
            "Cell \u001b[1;32mIn [69], line 16\u001b[0m, in \u001b[0;36mconvolve_n\u001b[1;34m(img, kernel, n)\u001b[0m\n\u001b[0;32m     13\u001b[0m convolved_img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(shape\u001b[39m=\u001b[39m(tgt_size, tgt_size))\n\u001b[0;32m     15\u001b[0m \u001b[39m# Iterate over the rows\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m, tgt_size \u001b[39m/\u001b[39;49m n, n):\n\u001b[0;32m     17\u001b[0m     \u001b[39m# Iterate over the columns\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, tgt_size \u001b[39m/\u001b[39m n, n):\n\u001b[0;32m     19\u001b[0m         \u001b[39m# img[i, j] = individual pixel value\u001b[39;00m\n\u001b[0;32m     20\u001b[0m         \u001b[39m# Get the current matrix\u001b[39;00m\n\u001b[0;32m     21\u001b[0m         mat \u001b[39m=\u001b[39m img[i:i\u001b[39m+\u001b[39mk, j:j\u001b[39m+\u001b[39mk]\n",
            "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
          ]
        }
      ],
      "source": [
        "######## Convolution with Laplacian Filter and the setting of stride=2 ##################\n",
        "\n",
        "def convolve_n(img: np.array, kernel: np.array, n) -> np.array:\n",
        "    # Assuming a rectangular image\n",
        "    tgt_size = calculate_target_size(\n",
        "        img_size=img.shape[0],\n",
        "        kernel_size=kernel.shape[0]\n",
        "    )\n",
        "    # To simplify things\n",
        "    k = kernel.shape[0]\n",
        "    \n",
        "    # 2D array of zeros\n",
        "    convolved_img = np.zeros(shape=(tgt_size, tgt_size))\n",
        "    \n",
        "    # Iterate over the rows\n",
        "    for i in range(0, tgt_size, n):\n",
        "        # Iterate over the columns\n",
        "        for j in range(0, tgt_size, n):\n",
        "            # img[i, j] = individual pixel value\n",
        "            # Get the current matrix\n",
        "            mat = img[i:i+k, j:j+k]\n",
        "            \n",
        "            # Apply the convolution - element-wise multiplication and summation of the result\n",
        "            # Store the result to i-th row and j-th column of our convolved_img array\n",
        "            convolved_img[i, j] = np.sum(np.multiply(mat, kernel))\n",
        "            \n",
        "    return convolved_img\n",
        "\n",
        "img_convolved2 = convolve_n(img=np.array(img_matrix), kernel=l_filter, n=2)\n",
        "\n",
        "print(img_convolved2[:10])\n",
        "\n",
        "print(img_convolved2.shape)\n",
        "\n",
        "plt.imshow(img_convolved2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2PT6NpSejuL"
      },
      "source": [
        "### 1.4 Implementation of MaxPooling\n",
        "\n",
        "Process the obtained array from the image with MaxPooling operation. **(15 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Prepare a 2X2 pooling mask.\n",
        "2. Conduct max pooing on image with prepared mask.\n",
        "3. **PRINT OUT** convolution result for first ten rows.\n",
        "4. **PRINT OUT** the shape of the convolution result.\n",
        "5. **DISPLAY** convolution result as image with matplotlib.(Don't worry about the value <0 or >255. Scaling process will be conducted in imshow function to make sure valid display.)\n",
        "\n",
        "**Hints:**\n",
        "1. You may just modify the centroid pool to top-left corner pool, and then, follow the same strategy above.\\\n",
        "    **Note: After the pooling, the size of the array is shrinked. You may think about the reason.**\n",
        "2. Validation for first 5X5 array (from upper-left corner), i.e., pooled_results[0:5,0:5].The example figure is below.\n",
        "\n",
        "[[ 98. 112.  93. 195. 173.]\\\n",
        " [ 84. 127. 137. 253. 254.]\\\n",
        " [ 85. 145. 225. 255. 242.]\\\n",
        " [104. 178. 216. 230. 242.]\\\n",
        " [ 95. 186. 147. 248. 242.]]\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1a18IWjrN0xHcp7bSNuj8kUM4JFFj3ebd)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOV72Sp9ejuL"
      },
      "outputs": [],
      "source": [
        "######## MaxPooling with the setting of 2X2 ##################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCxlJYlqejuM"
      },
      "source": [
        "## 2 - Convolution Neural Network ##\n",
        " \n",
        "In this section, we will use LeNet-5 (LeCun et al., 1998), one of representative deep nueral networks, to solve a  classification problem with Fashion MNIST benchmark.\n",
        "\n",
        "### 2.1 Library Preparation\n",
        "\n",
        "Import useful deep learning packages. \n",
        "\n",
        "**Tasks:**\n",
        "1. Import numpy and rename it to np.\n",
        "2. Import keras from tensorflow.\n",
        "3. Import layers from tensorflow.keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5enuEKHejuM"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUs3cBXXejuN"
      },
      "source": [
        "### 2.2 Training Data Preparation\n",
        "\n",
        "Import useful packages and prepare Fashion MNIST data. **(20 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Download Fashion MNIST data and split it with keras and prepare training/test data sets.\n",
        "2. Preprocess training/test data with normalization, dimension extension, and zero padding (for LeNet-5 configuration).\n",
        "3. Preprocess label data to binary class matrices.\n",
        "4. **PRINT OUT** first image in training set and its correponding label index\n",
        "5. **PRINT OUT** the shape of total training data, the number of training samples, and the number of test samples.\n",
        "\n",
        "**Hints**\n",
        "1. You may consider load function from the reference link. https://keras.io/api/datasets/ It provides dataloader function which can tackle downloading and data splitting automatically.\n",
        "2. For label preprocessing, you may consider **keras.utils.to_categorical** to convert class vectors to binary class matrices. This conversion makes sure the label can match the format of prediction output from neural network.\n",
        "3. For image display, consider showing the image and label **before dimension expansion and label preprocessing**.\n",
        "4. You may consider MNIST processing shown in class as an example.\n",
        "\n",
        "**References**\n",
        "- Fashion MNIST https://github.com/zalandoresearch/fashion-mnist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjyfYiGrejuN"
      },
      "outputs": [],
      "source": [
        "# # the data, split between train and test sets with keras.datasets\n",
        "\n",
        "# Model / data parameters\n",
        "# num_classes = ...\n",
        "# input_shape = (32, 32, 1) ## think about the reason\n",
        "\n",
        "# # Image Normalization (Scaling to [0, 1])\n",
        "# x_train = ... \n",
        "# x_test = ... \n",
        "\n",
        "# Print out first image and its correponding label index\n",
        "\n",
        "# # Dimension expansion to ensure that images have shape (28, 28, 1)\n",
        "# x_train = ...\n",
        "# x_test = ...\n",
        "\n",
        "# # Conduct padding on training/test images to (32, 32, 1) for LeNet-5\n",
        "# x_train = np.pad(x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "# x_test = np.pad(x_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
        "\n",
        "# # Print out the training/test data shapes and the numbers of training/test samples\n",
        "# print(\"x_train shape:\", x_train.shape)\n",
        "# print(x_train.shape[0], \"train samples\")\n",
        "# print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "# # convert label vectors to binary class matrices for training/test labels \n",
        "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "# y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw4GqzZmejuN"
      },
      "source": [
        "### 2.3 LeNet-5 \n",
        "\n",
        "Construct LeNet-5 as learning model for Fashion MNIST classification task. **(15 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Build up LeNet-5 with keras.Sequential\n",
        "2. Set the regularizer to l2 and regularizer lambda is **4e-5**.\n",
        "2. **PRINT OUT** the model summary.\n",
        "\n",
        "**Hints:**\n",
        "1. You may consider the convolution neural network shown in class as an example.\n",
        "2. The structure of LeNet-5 is listed below. Try to map each step to related processing operation. You can also search some materials to faciliate implementation. \n",
        "3. Some architecture settings are listed below. \n",
        "    - The kernel size for 2D convolution filter is **5 X 5**. You may think about the reason by calculation.\n",
        "    - Regularizer is set to L2 regularizer with **kernel_regularizer=regularizers.l2(4e-5)**.\n",
        "    - We change tanh activation to **\"relu\"** activation here. Please use **activation=\"relu\"** for implementation.\n",
        "    - We use MaxPooling instead of original AveragePooling. Please use \"**MaxPooling2D(pool_size=(2, 2))**\" for implementation.\n",
        "    - Please use **Flatten** to onvert 2D convolution layer to 1D fully connected layer.\n",
        "    - Gaussian connections are replaced with Softmax, and thus, the outputs are activated by Softmax function based on the number of classes.\n",
        "\n",
        "4. Validation result:\n",
        "    - Total params: 61,706\n",
        "    - Trainable params: 61,706\n",
        "    - Non-trainable params: 0\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Ks9RasENa0KiYRi2vwfJ_BQxkLB-x49R)\n",
        "\n",
        "\n",
        "**References:**\n",
        "- http://yann.lecun.com/exdb/lenet/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c97heiiSejuN"
      },
      "outputs": [],
      "source": [
        "### Construct LeNet-5\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# model = keras.Sequential(\n",
        "#     [\n",
        "#         keras.Input(shape=input_shape),\n",
        "#         layers.Conv2D(..., kernel_size=(...), kernel_regularizer=..., activation=...), #C1\n",
        "#         layers.MaxPooling2D(pool_size=(...)), # S2 Subsampling\n",
        "#         layers.Conv2D(..., kernel_size=(...), kernel_regularizer=..., activation=...), # C3\n",
        "#         layers.MaxPooling2D(pool_size=(...)), # S4 Subsampling\n",
        "#         layers.Flatten(), # Convert 2D convolution layer to 1D fully connected layer\n",
        "#         layers.Dense(..., kernel_regularizer=..., activation = ...), # C5\n",
        "#         layers.Dense(..., kernel_regularizer=..., activation = ...), # F6\n",
        "#         layers.Dense(num_classes, activation=...), # OUTPUT\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmVzUU9oejuO"
      },
      "source": [
        "#### 2.4 LeNet-5 Model Training\n",
        "\n",
        "Train LeNet-5 model with specific training strategy. **(20 Points)**\n",
        "\n",
        "**Tasks:**\n",
        "1. Set batch size to **64** for training. \n",
        "2. Pick **SGD optimizer** with learning rate of **0.1**, momentum of **0.9**, and **nesterov=True**, for model training.\n",
        "3. Pick **cross-entropy** loss function for optimization and evaluation metrics is set to **accuracy**.\n",
        "4. Set validation_split to **0.1** which means it excludes 1/10 training data for validation process.\n",
        "4. Train the model with **10 epochs**.\n",
        "5. Evaluate model with test data set and **PRINT OUT** : **test loss** and **test accuracy**. Note that the model here is the **LAST** model after **10 epochs** training.\n",
        "\n",
        "**Hints:**\n",
        "1. You may consider the examples from Keras to specify optimizer parameters. https://keras.io/api/optimizers/\n",
        "2. You may use the example shown in class to faciliate this implementation.\n",
        "3. You may see slightly different results every time you run the training. It is normal since there is randomness for training. However, you should expect the **BEST** validation accuracy is above **87%** which may not be the result from last epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz-Cvsu8ejuO"
      },
      "outputs": [],
      "source": [
        "### Train with SGD optimizer with learning rate =0.1, regularizer=4e-5, momentum=0.9, and nesterov=True\n",
        "\n",
        "# batch_size = ...\n",
        "# epochs = ...\n",
        "# sgd = keras.optimizers.SGD(lr=..., momentum=..., nesterov=...)\n",
        "# model.compile(loss=..., optimizer=..., metrics=[...])\n",
        "# history = model.fit(..., ..., batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi-xsLUiejuO"
      },
      "outputs": [],
      "source": [
        "### Print out the evaluation results, including test loss and test accuracy.\n",
        "\n",
        "# score = model.evaluate(..., ..., verbose=0)\n",
        "# print(\"Test loss:\", score[0])\n",
        "# print(\"Test accuracy:\", score[1])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Homework_03.ipynb",
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "91ccbeda090b25afe2cedac8d83ae6499ff9af9230ed9f00a7676b31a06712fb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
